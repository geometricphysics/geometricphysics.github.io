---
layout: master
title: Geometric Physics • Mathematical Methods • Representation Theory
---
<article>
  <h1>Representation Theory</h1>
  <section>
    <h3>Definitions</h3>
    <p>
      <ul>
        <li>
          <p>
            <span class="definition">A <b>representation</b> of a group is a collection of square non-singular matrices associated with the elements of the group and obeying the group multiplication rules.</span>
          </p>
        </li>
        <li>
          <p>
            <span class="definition">The number of rows or columns of a representation matrix is called the <b>dimension</b> of the representation.</span>
          </p>
        </li>
        <li>
          <p>
            <span class="definition">Representations are said to be <b>equivalent</b> if they are connected by a <b>similarity transformation</b>.</span>
          </p>
        </li>
        <li>
          <p>
            <span class="definition">A representation is said to be <b>reducible</b> if it is equivalent to a representation with diagonal block form.</span>
          </p>
        </li>
        <li>
          <p>
            <span class="definition">A representation is said to be <b>irreducible</b> if it is not possible to reduce all matrices representing the elements of the group into block forms by a similarity transformation.</span>
          </p>
        </li>
      </ul>
    </p>
  </section>
  <section>
    <h3>Can every finite group be represented by a group of matrices?</h3>
    <p>
      <span class="theorem">Every finite group is isomorphic to a matrix group, $Gl(n)$ for some $n$ large enough</span>. This is a consequence of Cayley's theorem.
    </p>
  </section>
  <section>
    <h3>Matrix Operations</h3>
    <p>
      Let $A (, B, C, \ldots)$ be a $n \times n$ matrix with entries $a_{ij} = (A)_{ij}$, where $i,j = 1,2, \dots, n$.
    </p>
    <p>
      We define the <span class="colloquial"><b>product</b> of two matrices $C = AB$</span> in terms of the entries
    </p>
    <p>
      $
      \begin{eqnarray}
      c_{ij} &amp; = &amp; \sum_{k} a_{ik} b_{kj}
      \end{eqnarray}
      $
    </p>
    <p>
      The <span class="colloquial"><b>complex conjugate</b> of a matrix $A$</span>, denoted by $\ComplexConjugate{A}$, has entries which are the complex conjugates of the corresponding entries of $A$:
    </p>
    <p>
      $
      \begin{eqnarray}
      (\ComplexConjugate{A})_{ij} &amp; = &amp; \ComplexConjugate{(a_{ij})}
      \end{eqnarray}
      $
    </p>
    <p>
      The <span class="colloquial"><b>transpose</b> of a matrix $A$</span>, denoted by $\Transpose{A}$, has its rows and columns interchanged with respect to $A$:
    </p>
    <p>
      $
      \begin{eqnarray}
      (\Transpose{A})_{ij} &amp; = &amp; a_{ji}
      \end{eqnarray}
      $
    </p>
    <p>
      The <span class="colloquial"><b>Hermitian conjugate</b> or <b>adjoint</b> of a matrix $A$</span>, denoted by $\Adjoint{A}$, is the transposed conjugate of $A$:
    </p>
    <p>
      $
      \begin{eqnarray}
      (\Adjoint{A})_{ij} &amp; = &amp; \ComplexConjugate{(a_{ji})}
      \end{eqnarray}
      $
    </p>
  </section>
  <section>
    <h3>Hermitian Matrices</h3>
    <p>
      We say that a <span class="colloquial">matrix $A$ is <b>Hermitian</b></span> precisely when
    </p>
    <p>
      $
      \begin{eqnarray}
      \Adjoint{A} &amp; = &amp; A
      \end{eqnarray}
      $
    </p>
  </section>
  <section>
    <h3>Orthogonal Matrices</h3>
    <p>
      We say that a <span class="colloquial">matrix $R$ is <b>Orthogonal</b></span> precisely when
    </p>
    <p>
      $
      \begin{eqnarray}
      \Transpose{R} R &amp; = &amp; R \Transpose{R} = I
      \end{eqnarray}
      $
    </p>
    <p>
      In terms of matrix components, this condition reads
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{k} r_{ki} r_{kj} &amp; = &amp; \sum_{k} r_{ik} r_{jk} = \delta_{ij}
      \end{eqnarray}
      $
    </p>
    <p>
      <span class="colloquial">rows (or columns) of an orthogonal matrix are mutually orthonormal</span>
    </p>
    <p>
      (taking the scalar product between vectors assuming a Euclidean metric).
    </p>
    <p>
      <span class="colloquial">An orthogonal transformation matrix leaves the scalar product of vectors invariant</span>:
    </p>
    <p>
      $
      \begin{eqnarray*}
      \ScalarProduct{\Primed{\Vector{u}}}{\Primed{\Vector{v}}}
      &amp; = &amp;
      \Transpose{(\Primed{\Vector{u}})} \Primed{\Vector{v}} \\
      &amp; = &amp;
      \Transpose{(R \Vector{u})} R \Vector{v} \\
      &amp; = &amp;
      \Transpose{\Vector{u}} \Transpose{R} R \Vector{v} \\
      &amp; = &amp;
      \Transpose{\Vector{u}} \Vector{v} \\
      &amp; = &amp;
      \ScalarProduct{\Vector{u}}{\Vector{v}}
      \end{eqnarray*}
      $
    </p>
  </section>
  <section>
    <h3>Unitary Matrices</h3>
    <p>
      We say that a <span class="colloquial">matrix $U$ is <b>unitary</b></span> precisely when
    </p>
    <p>
      $
      \begin{eqnarray}
      \Adjoint{U} U &amp; = &amp; U \Adjoint{U} = I
      \end{eqnarray}
      $
    </p>
    <p>
      This is a generalization of the orthonormal condition to complex matrices.
    </p>
    <p>
      In terms of matrix components, this condition reads
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{k} \ComplexConjugate{(r_{ki})} r_{kj} &amp; = &amp; \sum_{k} r_{ik} \ComplexConjugate{(r_{jk})} = \delta_{ij}
      \end{eqnarray}
      $
    </p>
    <p>
      <span class="colloquial">rows (or columns) of a unitary matrix are mutually orthonormal</span>
    </p>
    <p>
      (taking the appropriate scalar product between complex vectors).
    </p>
    <p>
      <span class="colloquial">A unitary transformation matrix leaves the scalar product of vectors invariant</span>:
    </p>
    <p>
      $
      \begin{eqnarray*}
      \BraKet{\Primed{u}}{\Primed{v}}
      &amp; = &amp;
      \Adjoint{(\Ket{\Primed{u}})} \Ket{\Primed{v}} \\
      &amp; = &amp;
      \Adjoint{(U \Ket{u})} U \Ket{v} \\
      &amp; = &amp;
      \Adjoint{\Ket{u}} \Adjoint{U} U \Ket{v} \\
      &amp; = &amp;
      \Adjoint{\Ket{u}} \Ket{v} \\
      &amp; = &amp;
      \BraKet{u}{v}
      \end{eqnarray*}
      $
    </p>
  </section>
  <section>
    <h3>Diagonalization of Hermitian Matrices</h3>
    <p>
      Standard proof for real eigenvalues and orthogonal eigenvectors.
    </p>
    <p>
      $
      \begin{eqnarray}
      H \Ket{\psi} &amp; = &amp; \lambda \Ket{\psi}
      \end{eqnarray}
      $
    </p>
    <p>
      Construct a unitary matrix $U$ from the normalized eigenvectors of $H$.
    </p>
    <p>
      $
      \begin{eqnarray}
      U &amp; = &amp; [\Ket{\psi_1} \ldots \Ket{\psi_n}]
      \end{eqnarray}
      $
    </p>
    <p>
      We can now write (being careful with the order of $U$ and $D$)
    </p>
    <p>
      $
      \begin{eqnarray}
      H U &amp; = &amp; U D
      \end{eqnarray}
      $
    </p>
    <p>
      where $D$ is the real diagonal matrix whose entries are the eigenvalues of $H$:
    </p>
    <p>
      $
      \begin{eqnarray}
      D
      &amp; = &amp;
      \left[
        \begin{array}{cccccccc}
            \lambda_1 &amp; 0         &amp; \cdots &amp; 0 \\
            0         &amp; \lambda_2 &amp; \cdots &amp; 0 \\
            \vdots    &amp; \vdots    &amp; \ddots &amp; \vdots \\
            0         &amp; 0         &amp; \cdots &amp; \lambda_n
        \end{array}
      \right]
      &amp; = &amp;
      \Adjoint{U} H U
      \end{eqnarray}
      $
    </p>
    <p>
      <span class="colloquial">Any Hermitian matrix can be diagonalized by an appropriate unitary transformation</span>.
    </p>
  </section>
  <section>
    <h3>Similarity Transformation</h3>
    <p>
      $
      \begin{eqnarray}
      A &amp; \mapsto &amp; \Inverse{S} A S
      \end{eqnarray}
      $
    </p>
    <p>
      where all matrices are $n \times n$ and matrix $S$ is invertible.
    </p>
  </section>
  <section>
    <h3>Transformation to Unitary Representations</h3>
    <p>
      <span class="theorem">Every representation is equivalent to a representation by unitary matrices.</span>
    </p>
    <p>
      <span class="theorem">i.e., Every representation can be brought into unitary form by a similarity transformation.</span>
    </p>
    <p>
      By <em>unitary form</em> we mean a representation of the same dimensionality as the original where <b>every matrix is unitary</b>.
    </p>
    <p>
      The basic idea is to construct a Hermitian matrix from the matrices of $G$. By diagonalizing $H$ we construct a unitary matrix $U$. Then construct a set of matrices from the original set, using $U$ and the eigenvalues of $H$ in a similarity transformation. Show that each matrix is unitary. Finally show that the transformed matrix is a representation of the original group.
    </p>
    <p>
      Let $\{A_1,\ldots,A_{\OrderOfGroup{G}}\}$ be a $d$-dimensional representation of an abstract group $G$.
    </p>
    <p>
      i.e., the $A_{\alpha}$ are a set of $\OrderOfGroup{G}$ $d \times d$ matrices with non-vanishing determinants (invertible).
    </p>
    <p>
      From these matrices we construct a manifestly Hermitian matrix
    </p>
    <p>
      $
      \begin{eqnarray}
      H &amp; = &amp; \sum_{\alpha = 1}^{\OrderOfGroup{G}} A_{\alpha} \Adjoint{(A_{\alpha})}
      \end{eqnarray}
      $
    </p>
    <p>
      Since $H$ is Hermitian, it can be diagonalized by a unitary transformation $U$, with $H U = U D$.
    </p>
    <p>
      $
      \begin{eqnarray*}
      D
      &amp; = &amp;
      \Adjoint{U} H U \\
      &amp; = &amp;
      \sum_{\alpha} \Adjoint{U} A_{\alpha} \Adjoint{(A_{\alpha})} U \\
      &amp; = &amp;
      \sum_{\alpha} \Adjoint{U} A_{\alpha} (U \Adjoint{U}) \Adjoint{(A_{\alpha})} U \\
      &amp; = &amp;
      \sum_{\alpha} \Adjoint{U} A_{\alpha} (U \Adjoint{U}) \Adjoint{(A_{\alpha})} U \\
      &amp; = &amp;
      \sum_{\alpha} (\Adjoint{U} A_{\alpha} U) \Adjoint{(\Adjoint{U} A_{\alpha} U)} \\
      &amp; = &amp;
      \sum_{\alpha} (X_{\alpha}) \Adjoint{(X_{\alpha})} \\
      \end{eqnarray*}
      $
    </p>
    <p>
      where
    <p>
    <p>
      $
      \begin{eqnarray}
      X_{\alpha} &amp; = &amp; \Adjoint{U} A_{\alpha} U
      \end{eqnarray}
      $
    </p>
    <p>
      In terms of components,
    </p>
    <p>
      $
      \begin{eqnarray*}
      D_{ij}
      &amp; = &amp;
      \sum_{\alpha} \sum_{k} (X_{\alpha})_{ik} (\Adjoint{(X_{\alpha})})_{kj} \\
      &amp; = &amp;
      \sum_{\alpha} \sum_{k} (X_{\alpha})_{ik} (\ComplexConjugate{(X_{\alpha})_{jk})} \\
      \end{eqnarray*}
      $
    </p>
    <p>
      A diagonal element is then
    </p>
    <p>
      $
      \begin{eqnarray*}
      D_{ii}
      &amp; = &amp;
      \sum_{\alpha} \sum_{k} (X_{\alpha})_{ik} (\ComplexConjugate{(X_{\alpha})_{ik})} \\
      &amp; = &amp;
      \sum_{\alpha} \sum_{k} \NormSquared{(X_{\alpha})_{ik}}
      \end{eqnarray*}
      $
    </p>
    <p>
      $D_{ii}$ must be positive because the sum over $k$ includes a diagonal element of $I$.
    </p>
    <p>
      We now construct
    </p>
    <p>
      $
      \begin{eqnarray}
      D^{1/2}
      &amp; = &amp;
      \left[
        \begin{array}{cccccccc}
            D_{11}^{1/2} &amp; 0            &amp; \cdots &amp; 0 \\
            0            &amp; D_{22}^{1/2} &amp; \cdots &amp; 0 \\
            \vdots       &amp; \vdots       &amp; \ddots &amp; \vdots \\
            0            &amp; 0            &amp; \cdots &amp; D_{nn}^{1/2}
        \end{array}
      \right]
      \end{eqnarray}
      $
    </p>
    <p>
      and
    </p>
    <p>
      $
      \begin{eqnarray}
      D^{-1/2}
      &amp; = &amp;
      \left[
        \begin{array}{cccccccc}
            D_{11}^{-1/2} &amp; 0             &amp; \cdots &amp; 0 \\
            0             &amp; D_{22}^{-1/2} &amp; \cdots &amp; 0 \\
            \vdots        &amp; \vdots        &amp; \ddots &amp; \vdots \\
            0             &amp; 0             &amp; \cdots &amp; D_{nn}^{-1/2}
        \end{array}
      \right]
      \end{eqnarray}
      $
    </p>
    <p>
      and form the matrices
    </p>
    <p>
      $
      \begin{eqnarray}
      B_{\alpha} &amp; = &amp; D^{-1/2} X_{\alpha} D^{1/2}
      \end{eqnarray}
      $
    </p>
    <p>
      together with the Hermitian conjugates
    </p>
    <p>
      $
      \begin{eqnarray}
      \Adjoint{B_{\alpha}} &amp; = &amp; D^{1/2} \Adjoint{X_{\alpha}} D^{-1/2}
      \end{eqnarray}
      $
    </p>
    <p>
      We now demonstrate that the $B_{\alpha}$ are unitary.
    </p>
    <p>
      $
      \begin{eqnarray}
      B_{\alpha} \Adjoint{B_{\alpha}}
      &amp; = &amp;
      D^{-1/2} X_{\alpha} D^{1/2} D^{1/2} \Adjoint{X_{\alpha}} D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} X_{\alpha} D \Adjoint{X_{\alpha}} D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} (\Adjoint{U} A_{\alpha} U) D (\Adjoint{U} \Adjoint{A_{\alpha}} U) D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} (\Adjoint{U} A_{\alpha}) (U D \Adjoint{U}) (\Adjoint{A_{\alpha}} U) D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} (\Adjoint{U} A_{\alpha}) H (\Adjoint{A_{\alpha}} U) D^{-1/2} \\
      &amp; = &amp;
      \sum_{\beta} D^{-1/2} (\Adjoint{U} A_{\alpha}) A_{\beta} \Adjoint{A_{\beta}} (\Adjoint{A_{\alpha}} U) D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} \left( \sum_{\beta} (\Adjoint{U} A_{\alpha}) A_{\beta} \Adjoint{A_{\beta}} (\Adjoint{A_{\alpha}} U) \right) D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} \left( \sum_{\beta} X_{\alpha} X_{\beta} \Adjoint{X_{\beta}} \Adjoint{X_{\alpha}} \right) D^{-1/2} \\
      \end{eqnarray}
      $
    </p>
    <p>
      Since the $A_{\alpha}$ are a representation of $G$, and since the $X_{\alpha}$ are related by a similarity transform to the $A_{\alpha}$, the $X_{\alpha}$ are also a representation of $G$. And since $G$ is a group, $X_{\alpha} X_{\beta} = X_{\gamma}$ is another matrix in this representation. The sum over $\beta$, by the Rearrangement Theorem, means that we are summing over a rearrangement of the $X$-representation and so we find
    </p>
    <p>
      $
      \begin{eqnarray}
      B_{\alpha} \Adjoint{B_{\alpha}}
      &amp; = &amp;
      D^{-1/2} \left( \sum_{\gamma} X_{\gamma} \Adjoint{X_{\gamma}} \right) D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} \left( \sum_{\gamma} (\Adjoint{U} A_{\gamma} U) (\Adjoint{U} \Adjoint{A_{\gamma}} U) \right) D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} \Adjoint{U} \left( \sum_{\gamma} A_{\gamma} \Adjoint{A_{\gamma}} \right) U D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} \Adjoint{U} H U D^{-1/2} \\
      &amp; = &amp;
      D^{-1/2} D D^{-1/2} \\
      &amp; = &amp;
      I
      \end{eqnarray}
      $
    </p>
    <p>
      Finally,
    </p>
    <p>
      $
      \begin{eqnarray}
      \Adjoint{B_{\alpha}} B_{\alpha}
      &amp; = &amp;
       D^{1/2} \Adjoint{X_{\alpha}} D^{-1/2} D^{-1/2} X_{\alpha} D^{1/2} \\
      &amp; = &amp;
      D^{1/2} \Adjoint{X_{\alpha}} D^{-1} X_{\alpha} D^{1/2} \\
      &amp; = &amp;
      D^{1/2} (\Adjoint{U} \Adjoint{A_{\alpha}} U) D^{-1} (\Adjoint{U} A_{\alpha} U) D^{1/2} \\
      &amp; = &amp;
      D^{1/2} (\Adjoint{U} \Adjoint{A_{\alpha}}) (U D^{-1} \Adjoint{U}) (A_{\alpha} U) D^{1/2} \\
      &amp; = &amp;
      ?
      \end{eqnarray}
      $
    </p>
    <p>
      Proving the reversed situation doesn't seem to follow the same proof! The assumption that the group elements are invertible is probably the key.
    </p>
    <p>
    </p>
    <p>
      $
      \begin{eqnarray}
      B_{\alpha}
      &amp; = &amp;
      D^{-1/2} \Adjoint{U } A_{\alpha} U D^{1/2} \\
      &amp; = &amp;
      \Adjoint{(U D^{-1/2})} A_{\alpha} U D^{1/2} \\
      &amp; = &amp;
      \Inverse{(U D^{1/2})} A_{\alpha} (U D^{1/2}) \QED \\
      \end{eqnarray}
      $
    </p>
  </section>
  <section>
    <h3>Schur's First Lemma (Commuting Matrix, One Representation)</h3>
    <p>
      <span class="theorem">Any non-zero matrix that commutes with all the matrices of an irreducable representation must be a constant multiple of the unit matrix.</span>
    </p>
    <p>
      The contrapositive is that if a non-constant commuting matrix exists then the representation is reducible.
    </p>
    <p>
    </p>
  </section>
  <section>
    <h3>Schur's Second Lemma (Matrix with Two Irreducible Representations)</h3>
    <span class="theorem">
    <p>
      Let $\{A_{1}, A_{2}, \ldots,A_{\OrderOfGroup{G}}\}$ and $\{\Primed{A}_1,\ldots,\Primed{A}_{\OrderOfGroup{G}}\}$ be two irreducible representations of a group $G$ of dimensions $d$ and $\Primed{d}$ respectively.
    </p>
    <p>
       If there is a matrix $M$ such that
    </p>
    <p>
      $
      \begin{eqnarray}
      M A_{\alpha} &amp; = &amp; \Primed{A}_{\alpha} M
      \end{eqnarray}
      $
    </p>
    <p>
      for $\alpha = 1, 2, \ldots, \OrderOfGroup{G}$,
    </p>
    <p>
      then
    </p>
    <p>
      if $d = \Primed{d}$, either $M = 0$ or the two representations differ by a similarity transformation, and
    </p>
    <p>
      if $d \neq \Primed{d}$, then $M = 0$.
    </p>
    </span>
  </section>
  <section>
    <h3>Great Orthogonality Theorem</h3>
    <span class="definition">
    <p>
      In this theorem it helps to see that vectors are constructed by having each matrix element from the group supply a single complex vector component. It follows that the number of components in the vector is $\OrderOfGroup{G}$. Is the dimensionality of this vector space $\OrderOfGroup{G}$? In this construction, for a given vector, all components are taken from the same row and column of each matrix in the group. We will use Dirac bra-ket notation to remind us that we are dealing with vectors over a complex field and to better bring out the orthogonality condition as the more geometric scalar product. If we use the superscript $k$ to denote the representation then
    </p>
    <p>
      $
      \begin{eqnarray}
      \Ket{V_{ij}^{k}}
      &amp; = &amp;
      \left[
        \begin{array}{cccccccc}
            (A_{1}^{k})_{ij}               \\
            (A_{2}^{k})_{ij}               \\
            \vdots                         \\
            (A_{\OrderOfGroup{G}}^{k})_{ij}
        \end{array}
      \right]
      \end{eqnarray}
      $
    </p>
    <p>
      Notice that each representation provides $d^2$ such vectors because the group element matrices are in $Mat(d, \Field{C})$.
    </p>
    </span>
    <span class="theorem">
    <p>
      Let $\{A_{1}, A_{2}, \ldots,A_{\OrderOfGroup{G}}\}$ and $\{\Primed{A}_1, \Primed{A}_2, \ldots,\Primed{A}_{\OrderOfGroup{G}}\}$ be <b>two</b> <b>inequivalent</b> <b>irreducible</b> representations of a group $G$ with elements $\{g_{1}, g_{2}, \ldots, g_{\OrderOfGroup{G}}\}$ and which have dimensions $d$ and $\Primed{d}$, respectively.
    </p>
    <p>
      The matrices $A_{\alpha}$ and $\Primed{A}_{\alpha}$ in the two representations correspond to the element $g_{\alpha}$ in $G$.
    </p>
    <p>
      Then
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{\alpha} \ComplexConjugate{(A_{\alpha})}_{ij} (\Primed{A}_{\alpha})_{i'j'}
      &amp; = &amp;
      \BraKet{V_{ij}}{\Primed{V}_{\Primed{i}\Primed{j}}}
      &amp; = &amp;
      0
      \end{eqnarray}
      $
    </p>
    <p>
      for all matrix elements.
    </p>
    <p>
      .i.e., <b>vectors</b> from <b>two</b> <b>inequivalent</b> <b>irreducible</b> representations are <b>orthogonal</b>.
    </p>
    <p>
      For the elements of a <b>single</b> <b>unitary</b> <b>irreducible</b> representation, we have
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{\alpha} \ComplexConjugate{(A_{\alpha})}_{ij} (A_{\alpha})_{i'j'}
      &amp; = &amp;
      \BraKet{V_{ij}}{V_{\Primed{i}\Primed{j}}}
      &amp; = &amp;
      \frac{\OrderOfGroup{G}}{d} \delta_{i,\Primed{i}} \delta_{j,\Primed{j}}
      \end{eqnarray}
      $
    </p>
    <p>
      .i.e., <b>vectors</b> from a <b>single</b> <b>unitary</b> <b>irreducible</b> representation satisfy an <b>orthogonality condition</b>.
    </p>
    <p>
      We can combine the two statements of the Great Orthogonality Theorem as
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{\alpha} \ComplexConjugate{(A^{k}_{\alpha})}_{ij} (A^{\Primed{k}}_{\alpha})_{i'j'}
      &amp; = &amp;
      \BraKet{V^{k}_{ij}}{V^{\Primed{k}}_{\Primed{i}\Primed{j}}}
      &amp; = &amp;
      \frac{\OrderOfGroup{G}}{d} \delta_{i,\Primed{i}} \delta_{j,\Primed{j}} \delta_{k,\Primed{k}}
      \end{eqnarray}
      $
    </p>
    </span>
  </section>
  <section>
    <h3>Immediate Consequences of the Great Orthogonality Theorem</h3>
    <p>
      In a $\OrderOfGroup{G}$-dimensional space there are at most $\OrderOfGroup{G}$ independent vectors. Suppose we have irreducible representations of dimension $d_{1}, d_{2}, \ldots, d_{N}$. Each representation with dimension $d_k$ furnishes $d_k^2$ vectors and so we must have
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{k} d_k^2 \leq \OrderOfGroup{G}
      \end{eqnarray}
      $
    </p>
    <p>
      This sets an upper bound for both the number and dimensionalities of representations. A finite group can only have a finite number of irreducible representations. This allows us to know whether we have found all the irreducible representations of a group.
    </p>
  </section>
</article>
