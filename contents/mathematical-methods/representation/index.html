---
layout: master
title: Geometric Physics • Mathematical Methods • Representation
---
<article>
  <h1>Representation</h1>
  <section>
    <h3>Big Ideas</h3>
    <p>
      The central theory of group representations is the <b>Great Orthogonality Theorem</b>.
    </p>
  </section>
  <section>
    <h3>Definitions</h3>
    <p>
      <ul>
        <li>
          <p>
            A <b>representation</b> of a group is a collection of square non-singular matrices associated with the elements of the group and obeying the group multiplication rules.
          </p>
        </li>
        <li>
          <p>
            The number of rows or columns of a representation matrix is called the <b>dimension</b> of the representation.
          </p>
        </li>
        <li>
          <p>
            Representations are said to be <b>equivalent</b> if they are connected by a <b>similarity transformation</b>.
          </p>
        </li>
        <li>
          <p>
            Equivalent matrices have the same trace.
          </p>
        </li>
        <li>
          <p>
            A representation is said to be <b>reducible</b> if it is equivalent to a representation with diagonal block form.
          </p>
        </li>
        <li>
          <p>
            A representation is said to be <b>irreducible</b> if it is not possible to reduce all matrices representing the elements of the group into block forms by a similarity transformation.
          </p>
        </li>
      </ul>
    </p>
  </section>
  <section>
    <h3>Matrix Operations</h3>
    <p>
      Let $A (, B, C, \ldots)$ be a $n \times n$ matrix with entries $a_{ij} = (A)_{ij}$, where $i,j = 1,2, \dots, n$.
    </p>
    <p>
      We define the <span class="colloquial"><b>product</b> of two matrices $C = AB$</span> in terms of the entries
    </p>
    <p>
      $
      \begin{eqnarray}
      c_{ij} &amp; = &amp; \sum_{k} a_{ik} b_{kj}
      \end{eqnarray}
      $
    </p>
    <p>
      The <span class="colloquial"><b>complex conjugate</b> of a matrix $A$</span>, denoted by $\ComplexConjugate{A}$, has entries which are the complex conjugates of the corresponding entries of $A$:
    </p>
    <p>
      $
      \begin{eqnarray}
      (\ComplexConjugate{A})_{ij} &amp; = &amp; \ComplexConjugate{(a_{ij})}
      \end{eqnarray}
      $
    </p>
    <p>
      The <span class="colloquial"><b>transpose</b> of a matrix $A$</span>, denoted by $\Transpose{A}$, has its rows and columns interchanged with respect to $A$:
    </p>
    <p>
      $
      \begin{eqnarray}
      (\Transpose{A})_{ij} &amp; = &amp; a_{ji}
      \end{eqnarray}
      $
    </p>
    <p>
      The <span class="colloquial"><b>Hermitian conjugate</b> or <b>adjoint</b> of a matrix $A$</span>, denoted by $\HermitianConjugate{A}$, is the transposed conjugate of $A$:
    </p>
    <p>
      $
      \begin{eqnarray}
      (\HermitianConjugate{A})_{ij} &amp; = &amp; \ComplexConjugate{(a_{ji})}
      \end{eqnarray}
      $
    </p>
  </section>
  <section>
    <h3>Hermitian Matrices</h3>
    <p>
      We say that a <span class="colloquial">matrix $A$ is <b>Hermitian</b></span> precisely when
    </p>
    <p>
      $
      \begin{eqnarray}
      \HermitianConjugate{A} &amp; = &amp; A
      \end{eqnarray}
      $
    </p>
  </section>
  <section>
    <h3>Orthogonal Matrices</h3>
    <p>
      We say that a <span class="colloquial">matrix $R$ is <b>Orthogonal</b></span> precisely when
    </p>
    <p>
      $
      \begin{eqnarray}
      \Transpose{R} R &amp; = &amp; R \Transpose{R} = I
      \end{eqnarray}
      $
    </p>
    <p>
      In terms of matrix components, this condition reads
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{k} r_{ki} r_{kj} &amp; = &amp; \sum_{k} r_{ik} r_{jk} = \delta_{ij}
      \end{eqnarray}
      $
    </p>
    <p>
      <span class="colloquial">rows (or columns) of an orthogonal matrix are mutually orthonormal</span>
    </p>
    <p>
      (taking the scalar product between vectors assuming a Euclidean metric).
    </p>
    <p>
      <span class="colloquial">An orthogonal transformation matrix leaves the scalar product of vectors invariant</span>:
    </p>
    <p>
      $
      \begin{eqnarray*}
      \ScalarProduct{\Primed{\Vector{u}}}{\Primed{\Vector{v}}}
      &amp; = &amp;
      \Transpose{(\Primed{\Vector{u}})} \Primed{\Vector{v}} \\
      &amp; = &amp;
      \Transpose{(R \Vector{u})} R \Vector{v} \\
      &amp; = &amp;
      \Transpose{\Vector{u}} \Transpose{R} R \Vector{v} \\
      &amp; = &amp;
      \Transpose{\Vector{u}} \Vector{v} \\
      &amp; = &amp;
      \ScalarProduct{\Vector{u}}{\Vector{v}}
      \end{eqnarray*}
      $
    </p>
  </section>
  <section>
    <h3>Unitary Matrices</h3>
    <p>
      We say that a <span class="colloquial">matrix $U$ is <b>unitary</b></span> precisely when
    </p>
    <p>
      $
      \begin{eqnarray}
      \HermitianConjugate{U} U &amp; = &amp; U \HermitianConjugate{U} = I
      \end{eqnarray}
      $
    </p>
    <p>
      This is a generalization of the orthonormal condition to complex matrices.
    </p>
    <p>
      In terms of matrix components, this condition reads
    </p>
    <p>
      $
      \begin{eqnarray}
      \sum_{k} \ComplexConjugate{(r_{ki})} r_{kj} &amp; = &amp; \sum_{k} r_{ik} \ComplexConjugate{(r_{jk})} = \delta_{ij}
      \end{eqnarray}
      $
    </p>
    <p>
      <span class="colloquial">rows (or columns) of a unitary matrix are mutually orthonormal</span>
    </p>
    <p>
      (taking the appropriate scalar product between complex vectors).
    </p>
    <p>
      <span class="colloquial">A unitary transformation matrix leaves the scalar product of vectors invariant</span>:
    </p>
    <p>
      $
      \begin{eqnarray*}
      \BraKet{\Primed{u}}{\Primed{v}}
      &amp; = &amp;
      \Adjoint{(\Ket{\Primed{u}})} \Ket{\Primed{v}} \\
      &amp; = &amp;
      \Adjoint{(U \Ket{u})} U \Ket{v} \\
      &amp; = &amp;
      \Adjoint{\Ket{u}} \Adjoint{U} U \Ket{v} \\
      &amp; = &amp;
      \Adjoint{\Ket{u}} \Ket{v} \\
      &amp; = &amp;
      \BraKet{u}{v}
      \end{eqnarray*}
      $
    </p>
  </section>
  <section>
    <h3>Diagonalization of Hermitian Matrices</h3>
    <p>
    </p>
  </section>
  <section>
    <h3>Lemma (Constant Matrix)</h3>
    <h4>Statement</h4>
    <p>
      Any representation by matrices with non-vanishing determinants is equivalent through a similarity transformation to a representation by unitary matrices.
    </p>
    <h4>Proof</h4>
    <p>
    </p>
  </section>
  <section>
    <h3>Schur's Lemma</h3>
    <h4>Statement</h4>
    <p>
      Any matrix that commutes with all the matrices of an irreducable representation must be a <b>constant matrix</b> (a multiple of the unit matrix).
    </p>
    <p>
      The contrapositive is that if a non-constant commuting matrix exists then the representation is reducible.
    </p>
    <h4>Proof</h4>
    <p>
    </p>
  </section>
  <section>
    <h3>Lemma (Irreducable Representation Transformations)</h3>
    <h4>Statement</h4>
    <p>
    </p>
    <h4>Proof</h4>
    <p>
    </p>
  </section>
  <section>
    <h3>Great Orthogonality Theorem</h3>
    <h4>Statement</h4>
    <p>
      All non-equivalent, irreducable, and unitary representations of a group satisfy the following relation:
    </p>
    <h4>Proof</h4>
    <p>
    </p>
  </section>
</article>
